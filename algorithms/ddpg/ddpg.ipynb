{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d68cbd5",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03003813",
   "metadata": {},
   "source": [
    "DDPG is a model-free (no transition probability) off-policy `actor-critic` algorithm that combines elements of policy gradient methods with deep Q-learning. DDPG is an extension of DQN for continuous action space. It uses `temporal difference learning` (bootstrapping) and `experience replay buffer` (off-policy) to learn the Q-value (represented by the Critic network). Unlike DQN, DDPG does not use $\\epsilon$-greedy policy (exploitation) for action selection. Rather, In DDPG, the behavior policy for action selection is derived from the actions generated by the Actor network (which is a deterministic target policy) with the addition of noise to encourage `exploration` in the environment.\n",
    "\n",
    "- DDPG uses four neural networks:\n",
    "    - The Actor network.\n",
    "    - The Critic network.\n",
    "    - The target Actor network.\n",
    "    - The target Critic network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffdb9a",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): DDPG**\n",
    "\n",
    "1. Initialize the environment to a random state $s_t$.\n",
    "\n",
    "2. Feed the current state $s_t$ to the Actor neural network that will return an action value $a_t$ (a continuous number, not a probability, since the policy is deterministic). \n",
    "\n",
    "3. Apply a noise (typically Gaussian) to the action $a_t$ to drive the agent in the environment that will return a reward $r_t$ and the next state $s_{t+1}$.\n",
    "\n",
    "4. At each time step, store the experience/transition as a tuple ($s_t, a_t, r_{t}, s_{t+1}, d_{t}$) into the replay buffer. Where $d_{t}$ is an optional Done (boolean) value to determine whether the episode ended. This is to ensure stability.\n",
    "\n",
    "5. Update Actor network:\n",
    "\n",
    "    5.1 Sample a random state from the memory buffer and feed it to the `Actor network` $\\mu$ to get the respective action value. This action value might be different than the ones stored in the buffer.\n",
    "    \n",
    "    5.2 Feed the previous state and action pair to the `Critic network` $Q$ to get the $Q(s_i, \\alpha_i | \\theta^{Q})$ value.\n",
    "    \n",
    "    5.3 Update Actor network's parameters $\\theta^{\\mu}$ by computing the `gradient ascent` of the Actor network loss function $J$ w.r.t the Actor parameters $\\theta^{\\mu}$:\n",
    "\n",
    "    \\begin{eqnarray}\n",
    "    \\theta^{\\mu}_{t+1} &=&  \\theta_t + \\alpha \\nabla_{\\theta^{\\mu}} J.\\\\\n",
    "    \\nabla_{\\theta^{\\mu}} J &=& \\mathbb{E}_{s_t \\sim \\rho^{\\beta}}[\\nabla_{\\theta^{\\mu}} Q(s, \\alpha | \\theta^{Q})|_{s=s_t, \\alpha=\\mu(s_t|\\theta^{\\mu})}].\n",
    "    \\end{eqnarray}\n",
    "\n",
    "    Where $\\theta^{\\mu}$ and $\\theta^{Q}$ represents the Actor and Critic network's parameters, respectively. And $\\nabla_{\\theta^{\\mu}} Q(s, \\alpha | \\theta^{Q})$ is the gradient of the Critic network w.r.t the Actor parameters. \n",
    "\n",
    "6. Update Critic network:\n",
    "\n",
    "    6.1 Sample a random mini-batch of state, new states, actions and rewards from the replay buffer.\n",
    "\n",
    "    6.2 Use `target Actor network` $\\mu'$ to get actions for new states.\n",
    "\n",
    "    6.3. Feed previous actions to the `target Critic network` $Q'$ to get the target value $y_i$.\n",
    "\n",
    "    6.4. Feed state and actions to the `Critic network` to get the predicted value $Q(s_i, \\alpha_i | \\theta^{Q})$. \n",
    "\n",
    "    6.5. Update the Critic network's parameters $\\theta^{Q}$ using `gradient descent` to minimize the mean squared error loss function of the Critic network:\n",
    "\n",
    "    $$\\frac{1}{N} \\sum_i (y_i - Q(s_i, \\alpha_i | \\theta^{Q}))^2.$$\n",
    "\n",
    "    Where $y_i = r_i + \\gamma Q'(s_{i+1}, \\mu'(s_{i+1}|\\theta^{\\mu'}| \\theta^{Q'})$ is the target value obtained in step 6.3.\n",
    "\n",
    "    6.6. Update the target networks using the soft update rule:\n",
    "\n",
    "    \\begin{align}\n",
    "    \\theta^{\\mu'} &= \\tau \\theta^{\\mu} (1-\\tau) \\theta^{\\mu'} . \\\\\n",
    "    \\theta^{Q'} &= \\tau \\theta^{Q} (1-\\tau) \\theta^{Q'}.\n",
    "    \\end{align}\n",
    "\n",
    "    Where $\\tau$ is a hyperparameter.\n",
    "\n",
    "7. Repeat until convergence.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
