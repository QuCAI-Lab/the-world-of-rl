{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cae3de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div align='center'>\n",
    "<h1> \n",
    "    <a href='https://arxiv.org/abs/1707.06347'> Proximal Policy Optimization (PPO) </a> \n",
    "</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1f289",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6ef35",
   "metadata": {},
   "source": [
    "Proximal Policy Optimization (PPO) is an Actor-Critic, online (learns from trajectories collected during runtime), and Policy Gradient algorithm, which means it is on-policy (uses only trajectories collected with the latest policy). It improves upon the TRPO algorithm. In TRPO, the KL constraint, which prevents the current policy $\\pi_{\\theta_{\\text{old}}}$ to be far from the updated policy $\\pi(\\theta)$, introduces an overhead that is circumvented in PPO by a clipped surrogate objective. Contrary to VPG and A2C/A3C algorithms, PPO indirectly optimizes the Policy performance objective $J(\\pi_{\\theta})$ and instead maximizes a surrogate objective function.\n",
    "\n",
    "PPO is suitable for both continuous and discrete action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc378c-2d61-4f85-8904-0b9caf11b2fc",
   "metadata": {},
   "source": [
    "## Actor loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc72ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "For the Actor network, the clipped surrogate objective loss function is defined as the minimum between two objectives, the no clipping or penalty objective which is the default in policy gradient methods, and its clipped version:\n",
    "\n",
    "$$L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) \\equiv \\hat{\\mathbb{E}}_t \\Bigg[\\text{min}\\Bigg(r_t(\\theta)A^{\\pi_{\\theta_k}}(s,a), \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a) \\Bigg) \\Bigg],$$\n",
    "\n",
    "where $\\epsilon$ is a parameter used to clip (constraint) the value of the probability ratio $$r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)},$$ in the range $1-\\epsilon$ to $1+\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4c73d-67aa-4c7d-807c-8d628c4c5544",
   "metadata": {},
   "source": [
    "The advantage function, denoted by $$A^{\\pi_{\\theta_k}}(s_t,a_t) = Q^{\\pi_{\\theta_k}}(s_t, a_t) - V^{\\pi_{\\theta_k}}(s_t),$$\n",
    "\n",
    "is the difference between the expected return (Action-Value function) and the baseline estimate. The baseline is the State Value function that gives a noisy estimate of the return. If the value of the advantage function is positive then the gradient is positive and the likelihood of the selected actions (action probabilities) increases, otherwise the gradient is negative and the actions are discouraged.\n",
    "\n",
    "This advantage is calculated as follows:\n",
    "\n",
    "$$A^{\\pi_{\\theta_k}}(s_t,a_t) = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\cdots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1},$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\delta_t = r_t + \\gamma V(s_{t+1})-V(s_t)$.\n",
    "- $\\gamma$ is the discount factor (~0.99).\n",
    "- $\\lambda$ is a parameter used to reduce variance (~0.95)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37190ee-5603-44a5-b4ea-fecbe50bfe99",
   "metadata": {},
   "source": [
    "A simplified version of the PPO-Clip objective, i.e., the Actor loss is [1]:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) =  \\hat{\\mathbb{E}}_t \\Bigg[\\text{min}\\Bigg(r_t(\\theta)A^{\\pi_{\\theta_k}}(s,a), g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a)) \\Bigg)\\Bigg],\n",
    "\\end{eqnarray}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "g(\\epsilon, A) = \n",
    "\\begin{cases}\n",
    "(1+\\epsilon)A, A \\geq 0. \\\\\n",
    "(1-\\epsilon)A, A < 0.\n",
    "\\end{cases}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11a71a8-48e8-4ff4-9772-f3960d1bab1e",
   "metadata": {},
   "source": [
    "## Critic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353d3e22-edc6-4a89-bdb9-04c83f333abf",
   "metadata": {},
   "source": [
    "The Critic network loss function is:\n",
    "\n",
    "$$L_{critic}^{\\text{VF}}(\\theta) = (V_{\\theta}(s_t) - V_t^{targ})^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2f7d1-92f6-40ce-b43c-89bf653c32dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Final loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5fedc-7580-416e-8cbe-71d7e368bf70",
   "metadata": {},
   "source": [
    "The final objective is the sum of the Critic loss and the Actor loss:\n",
    "\n",
    "$$L_t^{\\text{CLIP+VF+S}}(s, a, \\theta_k, \\theta) =: L_t^{\\text{PPO}}(s, a, \\theta_k, \\theta) = \\hat{\\mathbb{E}}_t \\Bigg[L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta) -c_1 L_{critic}^{VF}(\\theta)+c_2 S[\\pi_{\\theta}](s_t) \\Bigg],$$\n",
    "\n",
    "where $S$ is an entropy term used to ensure enough exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b0d80-e4db-4736-a487-ba12c0ee2af4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccaece-d009-4e72-af50-252b2c6cee69",
   "metadata": {},
   "source": [
    "The following cell presents a pseudocode for the PPO-Clip variant version of the PPO algorithm with the clipped objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a42062",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm (Pseudocode): Proximal Policy Optimization - Clip version (adapted from [Open AI](https://spinningup.openai.com/en/latest/algorithms/ppo.html#:~:text=PPO%20is%20an%20on%2Dpolicy,discrete%20or%20continuous%20action%20spaces.))**\n",
    "\n",
    "---\n",
    "\n",
    "- Input: initialize policy parameters $\\theta_0$, and initial value function parameters $\\phi_0$.\n",
    "\n",
    "- for iteration $= 0, 1, 2, \\dots$ do:\n",
    "\n",
    "    - for actor=$1,2,\\cdots, N$ do:\n",
    "\n",
    "        - Collect a set of trajectories $\\mathcal{H}_t \\doteq \\mathcal{D}_k\\doteq\\{\\tau_i\\} = (s_0, a_0, r_0, \\cdots , s_T, a_{T}, r_T)$ by executing the current policy $\\pi_k = \\pi(\\theta_k)=\\pi_{\\theta_{\\text{old}}}$ in the environment for $T$ time steps.\n",
    "        - For each trajectory, compute: \n",
    "            - the reward-to-go $\\hat{\\mathcal{R}}_t = \\sum_{t'=t}^T R(s_t', a_t', s_{t'+1})$, and\n",
    "            - the advantage estimates $\\hat{A}_1, \\dots, \\hat{A}_T$ (using any advantage estimation method) based on the current  on-policy state value function $V_{\\phi_k}$ used as the baseline: $$\\hat{A}_t = Q^{\\pi_{\\theta}}(s_t, a_t) - V^{\\pi_{\\theta}}(s_t) \\in {\\rm I\\!R}.$$ \n",
    "    - end for.\n",
    "    - Update the Policy by maximizing the PPO-Clip objective, typically via stochastic `gradient ascent` with Adam: $$\\theta_{k+1} =  \\text{arg max}_{\\theta } \\hat{\\mathbb{E}}_{s,a \\sim \\pi_{\\theta_k}} [L_\\text{actor}^{\\text{CLIP}}(s, a, \\theta_k, \\theta)] = \\text{arg max}_{\\theta}\\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\text{min } \\Bigg(\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_k}(a_t | s_t)}A^{\\pi_{\\theta_k}} (s_t, a_t), g(\\epsilon, A^{\\pi_{\\theta_k}} (s_t, a_t)) \\Bigg).$$\n",
    "    - Fit the value function by regression on mean-squared error, via some `gradient descent` algorithm, minimizing $(V_\\phi(s_t) - \\hat{\\mathcal{R}}_t)^2$ summed over all trajectories and time steps: \n",
    "    \n",
    "$$\\phi_{k+1} = \\underset{\\phi}{\\operatorname{arg\\,min}} \\ \\frac{1}{|\\mathcal{D}_k|T}\\sum_{\\tau \\in \\mathcal{D}_k} \\sum_{t=0}^T \\left(V_\\phi(s_t) - \\hat{\\mathcal{R}}_t \\right)^2.$$ \n",
    "\n",
    "    \n",
    "- end for.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e95330de-395a-4ba5-bf89-33f9546b4757",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a87ccd3-daab-46bb-9faa-b8624e6d1cdd",
   "metadata": {},
   "source": [
    "[1] https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611a320-4126-482b-a6bf-8834b195a568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a2e61",
   "metadata": {},
   "source": [
    "Following the paper's implementation, the Policy network is an MLP with two hidden layers, 64 neurons, and tanh activation function. The $c_1$ coefficient in the final objective function (critic loss + actor loss) is discarted since the policy and the value networks do not share parameters between each other. And the entropy term used to ensure enough exploration is also ignored.\n",
    "\n",
    "Here, we define:\n",
    "\n",
    "$$L_t^{\\text{VF}}(\\theta) =: L_{critic} \\equiv MSE(\\hat{A}_t(s_t,a_t) + CriticValue_{mem} - CriticValue_{net}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aac0014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
